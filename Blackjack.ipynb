{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d7f507",
   "metadata": {},
   "source": [
    "# Assignment 10 - 705.603 - Local Blackjack Optimization \n",
    "\n",
    "Additionally, there is a bug in the `watch_trained_agent` function.  The call to `get_action` should not use an epsilon value as there is no need to explore randomly while watching the trained agent.  Added a new function call `watch_trained_agent_no_explore` and you can see the improved function of the test from a rewad of -40 after 100 episodes to a reward of -25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8ea71",
   "metadata": {},
   "source": [
    "## Blackjack\n",
    "\n",
    "### Environment Details\n",
    "\n",
    "    ### Action Space\n",
    "    There are two actions: stick (0), and hit (1).\n",
    "    \n",
    "    ### Observation Space\n",
    "    Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
    "    The observation consists of a 3-tuple containing: \n",
    "        1. the player's current sum\n",
    "        2. the value of the dealer's one showing card (1-10 where 1 is ace)\n",
    "        3. whether the player holds a usable ace (0 or 1).\n",
    "        \n",
    "    ### Rewards\n",
    "    - win game: +1\n",
    "    - lose game: -1\n",
    "    - draw game: 0\n",
    "    - win game with natural blackjack:\n",
    "        +1.5 (if natural is True)\n",
    "        +1 (if natural is False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428df6f",
   "metadata": {},
   "source": [
    "# Install needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U gym\n",
    "! pip install -U torch\n",
    "! pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3a2c0",
   "metadata": {},
   "source": [
    "# Import libraries and Create Class to Display Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "class JupyterDisplay(object):\n",
    "    def __init__(self, figsize: tuple):\n",
    "        self.figsize = figsize\n",
    "        self.mode = \"rgb_array\"\n",
    "    \n",
    "    def show(self, env):\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        plt.imshow(env.render())               # Removed render mode for compatibility\n",
    "        plt.axis('off')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d7c93",
   "metadata": {},
   "source": [
    "# Create Blackjack Environment and test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e41e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03870217",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97046065",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ad0cb",
   "metadata": {},
   "source": [
    "# Q-Learning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1af5da",
   "metadata": {},
   "source": [
    "## function `get_state_idxs`\n",
    "\n",
    "This is a function that retrieves the three values from a returned state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fd0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_idxs(state):\n",
    "    idx1, idx2, idx3 = state\n",
    "    idx3 = int(idx3)\n",
    "    return idx1, idx2, idx3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc9859",
   "metadata": {},
   "source": [
    "## function `update_qtable`\n",
    "\n",
    "Function that uses the Bellman equation to update a qtable given a learning rate and a discount faactor. \n",
    "\n",
    "Arguments:\n",
    "- qtable - numpy array with dimension for each of three state description lus a dimension for each action\n",
    "- state - tuple of shape (idx1, idx2, idx3) where idx 1 and 2 are integers and idx 3 is a bool. idx 1 2 and 3 are described above.\n",
    "- action - int represeting Hit(1) or Stick(0)\n",
    "- reward - float representing reward for a given step\n",
    "- next_state - the next state resulting from action.\n",
    "- alpha - learning rate\n",
    "- gamma - discount factor\n",
    "\n",
    "Returns: Updated qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daba6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_qtable(qtable, state, action, reward, next_state, alpha, gamma):\n",
    "    curr_idx1, curr_idx2, curr_idx3 = get_state_idxs(state)\n",
    "    next_idx1, next_idx2, next_idx3 = get_state_idxs(next_state)\n",
    "    curr_state_q = qtable[curr_idx1][curr_idx2][curr_idx3]\n",
    "    next_state_q = qtable[next_idx1][next_idx2][next_idx3]\n",
    "    qtable[curr_idx1][curr_idx2][curr_idx3][action] += \\\n",
    "            alpha * (reward + gamma * np.max(next_state_q) - curr_state_q[action])\n",
    "    return qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702fcbe",
   "metadata": {},
   "source": [
    "## function `get_action`\n",
    "\n",
    "Function that obatins the action to be taken as either exploration or exploitation\n",
    "\n",
    "Arguments:\n",
    "- qtable - numpy array with dimension for each of three state description lus a dimension for each action\n",
    "- state - tuple of shape (idx1, idx2, idx3) where idx 1 and 2 are integers and idx 3 is a bool. idx 1 2 and 3 are described above.\n",
    "- epsilon - float representing likelihood of action being random exploration.\n",
    "\n",
    "Returns: int for action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e97deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(qtable, state, epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        idx1, idx2, idx3 = get_state_idxs(state)\n",
    "        action = np.argmax(qtable[idx1][idx2][idx3])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6dc8c0",
   "metadata": {},
   "source": [
    "## function `train_agent`\n",
    "\n",
    "Function that develops q table by playing game with passed parameters.  This function required some modification as that original was for an older version of gym.  Mods are noted below.\n",
    "\n",
    "Arguments:\n",
    "- env - The blackjack gym environment\n",
    "- qtable - numpy array with dimension for each of three state description plus a dimension for each action\n",
    "- num_episodes - int for number of games to play\n",
    "- alpha - float learning rate\n",
    "- gamma - float discount factor\n",
    "- epsilon - float representing likelihood of action being random exploration.\n",
    "- epsilon_decay - float rate of decline of epsilon\n",
    "\n",
    "Returns: qtable for trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env,\n",
    "                qtable: np.ndarray,\n",
    "                num_episodes: int,\n",
    "                alpha: float, \n",
    "                gamma: float, \n",
    "                epsilon: float, \n",
    "                epsilon_decay: float) -> np.ndarray:\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()                                     # Added blank for extra returned argument\n",
    "        done = False\n",
    "        while True:\n",
    "            action = get_action(qtable, state, epsilon)\n",
    "            new_state, reward, done, _, info = env.step(action)    # Added blank for extra returned argument\n",
    "            qtable = update_qtable(qtable, state, action, reward, new_state, alpha, gamma)\n",
    "            state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        epsilon = np.exp(-epsilon_decay*episode)\n",
    "    return qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb02ee6",
   "metadata": {},
   "source": [
    "## function `watch_trained_agent`\n",
    "\n",
    "Function that plays blackjack with provided qtable as policy.  This function required some modification as that original was for an older version of gym.  Mods are noted below.\n",
    "\n",
    "Arguments:\n",
    "- env - The blackjack gym environment\n",
    "- qtable - numpy array with dimension for each of three state description plus a dimension for each action\n",
    "- num_rounds - int for number of games to play\n",
    "\n",
    "Returns: list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b6096",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (8,4)\n",
    "\n",
    "def watch_trained_agent(env, qtable, num_rounds):\n",
    "    envdisplay = JupyterDisplay(figsize=FIGSIZE)\n",
    "    rewards = []\n",
    "    for s in range(1, num_rounds+1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        round_rewards = 0\n",
    "        while True:\n",
    "            action = get_action(qtable, state, epsilon)          \n",
    "            new_state, reward, done, _, info = env.step(action)  # Added blank for extra returned argument\n",
    "            envdisplay.show(env)\n",
    "\n",
    "            round_rewards += reward\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "        rewards.append(round_rewards)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ae3a0",
   "metadata": {},
   "source": [
    "## function `watch_trained_agent_no_exploration`\n",
    "\n",
    "Function that plays blackjack with provided qtable as policy.  This function required soem modification as that original was for an older version of gym.  Mods are noted below.  This function has epsilon set to zero.\n",
    "\n",
    "Arguments:\n",
    "- env - The blackjack gym environment\n",
    "- qtable - numpy array with dimension for each of three state description lus a dimension for each action\n",
    "- num_rounds - int for number of games to play\n",
    "\n",
    "Returns: list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53043243",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (8,4)\n",
    "\n",
    "def watch_trained_agent_no_exploration(env, qtable, num_rounds):\n",
    "    envdisplay = JupyterDisplay(figsize=FIGSIZE)\n",
    "    rewards = []\n",
    "    for s in range(1, num_rounds+1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        round_rewards = 0\n",
    "        while True:\n",
    "            action = get_action(qtable, state, 0)                # epsilon set to 0\n",
    "            new_state, reward, done, _, info = env.step(action)  # Added blank for extra returned argument\n",
    "            envdisplay.show(env)\n",
    "\n",
    "            round_rewards += reward\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "        rewards.append(round_rewards)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b105a",
   "metadata": {},
   "source": [
    "# Q-Learning Execution\n",
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(qtable):\n",
    "    print('PC DC Soft Pol')\n",
    "    dim1, dim2, dim3, dim4 = qtable.shape\n",
    "    for player_count in range(10,21):\n",
    "        for dealer_card in range(dim2):\n",
    "            for soft in range(dim3):\n",
    "                q_stay = qtable[player_count, dealer_card, soft, 0]\n",
    "                q_hit  = qtable[player_count, dealer_card, soft, 1]\n",
    "                pol = \"Stay\" if q_stay>=q_hit else \"Hit\"\n",
    "                print(player_count+1, dealer_card+1, soft, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed723d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_policy(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "# get initial state\n",
    "state = env.reset()\n",
    "\n",
    "state_size = [x.n for x in env.observation_space]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "qtable = np.zeros(state_size + [action_size]) #init with zeros\n",
    "\n",
    "\n",
    "alpha = 0.3 # learning rate\n",
    "gamma = 0.1 # discount rate\n",
    "epsilon = 0.9     # probability that our agent will explore\n",
    "decay_rate = 0.005\n",
    "\n",
    "# training variables\n",
    "num_hands = 500_000\n",
    "\n",
    "qtable = train_agent(env,\n",
    "                     qtable,\n",
    "                     num_hands,\n",
    "                     alpha,\n",
    "                     gamma,\n",
    "                     epsilon,\n",
    "                     decay_rate)\n",
    "\n",
    "print(f\"Qtable Max: {np.max(qtable)}\")\n",
    "print(f\"Qtable Mean: {np.mean(qtable)}\")\n",
    "print(f\"Qtable Num Unique Vals: {len(np.unique(qtable))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762fcfb",
   "metadata": {},
   "source": [
    "## Test as provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9379c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Watch trained agent\n",
    "env = gym.make(\"Blackjack-v1\", render_mode='rgb_array')    # Added render mode for compatibility\n",
    "#env.action_space.seed(42)\n",
    "rewards = watch_trained_agent(env, qtable, num_rounds=100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output reward over hands played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.cumsum(rewards))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.title(\"Total Rewards Over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895b978",
   "metadata": {},
   "source": [
    "## Test without exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc119249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Watch trained agent\n",
    "env = gym.make(\"Blackjack-v1\", render_mode='rgb_array')    # Added render mode for compatibility\n",
    "#env.action_space.seed(42)\n",
    "rewards = watch_trained_agent_no_exploration(env, qtable, num_rounds=100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output reward over hands played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.cumsum(rewards))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.title(\"Total Rewards Over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6252c",
   "metadata": {},
   "source": [
    "# DRL Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5213a93",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b208d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ca291",
   "metadata": {},
   "source": [
    "## Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150070a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e3)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 3e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e5931",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcae4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    -------\n",
    "    Neural Network Used for Agent to Approximate Q-Values\n",
    "    -------\n",
    "    [Params]\n",
    "        'state_size' -> size of the state space\n",
    "        'action_size' -> size of the action space\n",
    "        'seed' -> used for random module\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756aafd1",
   "metadata": {},
   "source": [
    "## Define Agent Class\n",
    "\n",
    "This class creates local and target networks and provides methids for activities needed for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    --------\n",
    "    Deep Q-Learning Agent\n",
    "    --------\n",
    "    [Params]\n",
    "        'state_size' -> size of the state space\n",
    "        'action_size' -> size of the action space\n",
    "        'seed' -> used for random module\n",
    "    --------\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"\n",
    "        --------------\n",
    "        Take an action given the current state (S(i))\n",
    "        --------------\n",
    "        [Params]\n",
    "            'state' -> current state\n",
    "            'eps' -> current epsilon value\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state).cpu().data.numpy()\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values), np.max(action_values)\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model: nn.Module, target_model: nn.Module, tau: float):\n",
    "        \"\"\"\n",
    "        --------\n",
    "        Update our target network with the weights from the local network\n",
    "        --------\n",
    "        Formula for each param (w): w_target = τ*w_local + (1 - τ)*w_target\n",
    "        See https://arxiv.org/pdf/1509.02971.pdf\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000364da",
   "metadata": {},
   "source": [
    "## Create Memeory Deque Class\n",
    "\n",
    "An instance of this class is used as a recall buffer for DRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    ------------\n",
    "    Used to store agent experiences\n",
    "    ------------\n",
    "    [Params]\n",
    "        'action_size' -> length of the action space\n",
    "        'buffer_size' -> Max size of our memory buffer\n",
    "        'batch_size' -> how many memories to randomly sample\n",
    "        'seed' -> seed for random module\n",
    "    ------------\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ad987",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07790846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def dqn(n_episodes=2000, eps_start=0.99, eps_end=0.02, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    -------------\n",
    "    Train a Deep Q-Learning Agent\n",
    "    -------------\n",
    "    [Params]\n",
    "        'n_episodes' -> number of episodes to train for\n",
    "        'eps_start' -> epsilon starting value\n",
    "        'eps_end' -> epsilon minimum value\n",
    "        'eps_decay' -> how much to decrease epsilon every iteration\n",
    "    -------------\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                   \n",
    "    \n",
    "    for episode in range(1, n_episodes+1):\n",
    "        done = False\n",
    "        episode_score = 0\n",
    "        \n",
    "        state, _ = env.reset()                                 # Added _ for new version of gym\n",
    "        state = np.array(get_state_idxs(state), dtype=float)\n",
    "        state[0] = state[0]/32\n",
    "        state[1] = state[1]/10\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state, eps)\n",
    "            if isinstance(action, tuple):\n",
    "                action, value = action\n",
    "            else:\n",
    "                value = 1.\n",
    "            next_state, reward, done, _, _ = env.step(action)   # Added second _ for new version of gym\n",
    "            reward *= value\n",
    "            next_state = np.array(get_state_idxs(next_state), dtype=float)\n",
    "            next_state[0] = next_state[0]/32\n",
    "            next_state[1] = next_state[1]/10\n",
    "        \n",
    "            agent.step(state, action, reward, next_state, done)   \n",
    "            state = next_state\n",
    "            episode_score += reward\n",
    "        \n",
    "        scores_window.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "            \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)), end=\"\")\n",
    "        if episode % 5000 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "agent = Agent(state_size=3, action_size=2, seed=0)\n",
    "scores = dqn(n_episodes=70_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e765fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", render_mode='rgb_array')    # Added render mode for compatibility\n",
    "envdisplay = JupyterDisplay(figsize=(10,6))\n",
    "\n",
    "num_hands = 100\n",
    "\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "results = []\n",
    "for hand in range(num_hands):\n",
    "    state, _ = env.reset()                                   # Added _ for new version of gym\n",
    "    state = np.array(get_state_idxs(state), dtype=float)\n",
    "    state[0] = state[0]/32\n",
    "    state[1] = state[1]/10\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame = env.render()        \n",
    "        action = agent.act(state)\n",
    "        if isinstance(action, tuple):\n",
    "            action, value = action\n",
    "        else:\n",
    "            value = 1.\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)         # Added second _ for new version of gym\n",
    "        reward *= value\n",
    "        state = np.array(get_state_idxs(state), dtype=float)\n",
    "        state[0] = state[0]/32\n",
    "        state[1] = state[1]/10\n",
    "\n",
    "    envdisplay.show(env)\n",
    "    results.append(reward)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c255101",
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_avg = np.argwhere(np.array(results) > 0).size / len(results)\n",
    "print(f\"Batting Average: {batting_avg*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf37d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.cumsum(results))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.title(\"Total Rewards Over Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292df683",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cash = 100.\n",
    "\n",
    "pct_gain = ((start_cash + np.sum(results)*1000) - 100) / 100\n",
    "print(f\"Percent Gain: {pct_gain*100:.2f}%\")\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(start_cash + np.cumsum(results)*1000, c=\"g\")\n",
    "plt.ylabel(\"Cash ($)\")\n",
    "plt.xlabel(\"Dealt Hands\")\n",
    "plt.title(f\"Total Cash Over Time | Starting Cash: ${int(start_cash)} | Win Pct: {batting_avg:.2f}%\", c=\"darkgreen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c85bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
